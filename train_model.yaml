name: Forward_forward_training
description: Compiles and trains the FFN model using the training dataset and saves the trained model.
inputs:
  - {name: ff_model, type: Model}
  - {name: train_dataset, type: Dataset}
  - {name: num_epochs, type: Integer}
outputs:
  - {name: trained_model, type: Model}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet tensorflow keras || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet tensorflow keras --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import pickle
        import os
        import tensorflow as tf
        from keras.optimizers import Adam
        from keras.models import load_model
        import glob
        import tarfile
        import zipfile
        import json
        from keras.models import model_from_json
        import argparse
        import pickle
        import os
        import tensorflow as tf
        import keras
        from keras import ops
        from tensorflow.compiler.tf2xla.python import xla

        # parser = argparse.ArgumentParser()
        # parser.add_argument('--ff_model', type=str, required=True)
        # args = parser.parse_args()

        class FFDense(keras.layers.Layer):
            def __init__(self, units, init_optimizer, loss_metric, num_epochs=50,
                         use_bias=True, kernel_initializer="glorot_uniform",
                         bias_initializer="zeros", kernel_regularizer=None,
                         bias_regularizer=None, **kwargs):
                super().__init__(**kwargs)
                self.dense = keras.layers.Dense(
                                                units,
                                                activation=None,  # or explicitly specify if needed
                                                use_bias=use_bias,
                                                kernel_initializer=kernel_initializer,
                                                bias_initializer=bias_initializer,
                                                kernel_regularizer=kernel_regularizer,
                                                bias_regularizer=bias_regularizer
                                                )
                self.relu = keras.layers.ReLU()
                self.optimizer = init_optimizer()
                self.loss_metric = loss_metric
                self.threshold = 1.5
                self.num_epochs = num_epochs

            def call(self, x):
                x_norm = ops.norm(x, ord=2, axis=1, keepdims=True) + 1e-4
                x_dir = x / x_norm
                res = self.dense(x_dir)
                return self.relu(res)

            def forward_forward(self, x_pos, x_neg):
                for _ in range(self.num_epochs):
                    with tf.GradientTape() as tape:
                        g_pos = ops.mean(ops.power(self.call(x_pos), 2), 1)
                        g_neg = ops.mean(ops.power(self.call(x_neg), 2), 1)
                        loss = ops.log(1 + ops.exp(ops.concatenate([-g_pos + self.threshold,
                                                                    g_neg - self.threshold], 0)))
                        mean_loss = ops.cast(ops.mean(loss), dtype="float32")
                        self.loss_metric.update_state([mean_loss])
                    grads = tape.gradient(mean_loss, self.dense.trainable_weights)
                    self.optimizer.apply_gradients(zip(grads, self.dense.trainable_weights))
                return ops.stop_gradient(self.call(x_pos)), ops.stop_gradient(self.call(x_neg)), self.loss_metric.result()

        class FFNetwork(keras.Model):
            def __init__(self, dims, init_layer_optimizer=lambda: keras.optimizers.Adam(0.03), **kwargs):
                super().__init__(**kwargs)
                self.init_layer_optimizer = init_layer_optimizer
                self.loss_var = keras.Variable(0.0, trainable=False, dtype="float32")
                self.loss_count = keras.Variable(0.0, trainable=False, dtype="float32")
                self.layer_list = [keras.Input(shape=(dims[0],))]
                self.metrics_built = False
                for d in range(len(dims) - 1):
                    self.layer_list.append(
                        FFDense(dims[d + 1],
                                init_optimizer=self.init_layer_optimizer,
                                loss_metric=keras.metrics.Mean())
                    )

            @tf.function(reduce_retracing=True)
            def overlay_y_on_x(self, data):
                X_sample, y_sample = data
                max_sample = ops.cast(ops.amax(X_sample, axis=0, keepdims=True), dtype="float64")
                X_zeros = ops.zeros([10], dtype="float64")
                X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])
                X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])
                return X_sample, y_sample

            @tf.function(reduce_retracing=True)
            def predict_one_sample(self, x):
                goodness_per_label = []
                x = ops.reshape(x, [ops.shape(x)[0] * ops.shape(x)[1]])
                for label in range(10):
                    h, _ = self.overlay_y_on_x((x, label))
                    h = ops.reshape(h, [-1, ops.shape(h)[0]])
                    goodness = []
                    for layer in self.layer_list[1:]:
                        h = layer(h)
                        goodness.append(ops.mean(ops.power(h, 2), 1))
                    goodness_per_label.append(ops.expand_dims(ops.sum(goodness, keepdims=True), 1))
                return ops.cast(ops.argmax(tf.concat(goodness_per_label, 1), 1), dtype="float64")

            def update_pierce_params(self, pierce_params):
                self.optimiser = pierce_params.get("optimiser", keras.optimizers.Adam(learning_rate=0.03))
                self.threshold = pierce_params.get("threshold", 1.5)
                for layer in self.layer_list:
                    if hasattr(layer, "optimizer"):
                        layer.optimizer = self.optimiser

            def predict(self, data):
                return ops.vectorized_map(self.predict_one_sample, data).numpy().astype(int)

            @tf.function(jit_compile=False)
            def train_step(self, data):
                x, y = data
                if not self.metrics_built:
                    for metric in self.metrics:
                        if hasattr(metric, "build"):
                            metric.build(y, y)
                    self.metrics_built = True
                x = ops.reshape(x, [-1, ops.shape(x)[1] * ops.shape(x)[2]])
                x_pos, y = ops.vectorized_map(self.overlay_y_on_x, (x, y))
                random_y = tf.random.shuffle(y)
                x_neg, _ = tf.map_fn(self.overlay_y_on_x, (x, random_y))
                h_pos, h_neg = x_pos, x_neg
                for layer in self.layers:
                    if isinstance(layer, FFDense):
                        h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)
                        self.loss_var.assign_add(loss)
                        self.loss_count.assign_add(1.0)
                    else:
                        h_pos = layer(h_pos)
                return {"FinalLoss": self.loss_var / self.loss_count}


        
        parser = argparse.ArgumentParser()
        parser.add_argument('--ff_model', type=str, required=True)
        parser.add_argument('--train_dataset', type=str, required=True)
        # parser.add_argument('--num_epochs', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        args = parser.parse_args()

        # Load model
        
        # output_files = glob.glob("/tmp/*")
        # print("Files in /tmp:")
        # for f in output_files:
        #     print(f)
        
        print(f"the ff_model path is {args.ff_model}")
        # tgz_path = args.ff_model
        
        # zip_path = args.ff_model 
        # with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        #     print("Files in archive:")
        #     zip_ref.printdir() 
        #     for name in zip_ref.namelist():
        #         print(name)


        
        
        zip_path = args.ff_model
        extract_path = "/tmp/ff_model_extracted"
        
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        
        with open(os.path.join(extract_path, "config.json"), "r") as f:
            model_json = f.read()
        
        # model = model_from_json(model_json)
        model = FFNetwork([784, 500, 500])
        model.build(input_shape=(None, 784))
        model.load_weights(os.path.join(extract_path, "model.weights.h5"))

        # with tarfile.open(tgz_path, "r:gz") as tar:
        #     print("Files in archive:")
        #     for member in tar.getmembers():
        #         print(member.name)
        
        # args.ff_model += ".keras"
        # model = load_model(args.ff_model)
        
        # Load dataset
        # with open(args.num_epochs, "rb") as f: num_epochs = pickle.load(f)
        train_data = tf.data.experimental.load(args.train_dataset)

        # Compile and train
        model.compile(
            optimizer=Adam(learning_rate=0.03),
            loss="mse",
            jit_compile=False,
            metrics=[]
        )
        model.fit(train_data, epochs=num_epochs)

        # # Save the trained model
        # os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        # with open(args.trained_model, "wb") as f:
        #     pickle.dump(model, f)

        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        model.save(args.trained_model + ".keras")   # args.ff_model should be a directory path
        os.rename(args.trained_model + ".keras", args.trained_model)
    args:
      - --ff_model
      # - /tmp/outputs/ff_model/data.keras
      - {inputPath: ff_model}
      - --train_dataset
      - {inputPath: train_dataset}
    #   - --num_epochs
      - {inputValue: num_epochs}
      - --trained_model
      - {outputPath: trained_model}
      # - /tmp/outputs/trained_model/data.keras
